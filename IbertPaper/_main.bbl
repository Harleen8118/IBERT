\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{ARM}(2020)]{armcortexm}
{ARM}.
\newblock {Cortex-M},
  https://developer.arm.com/ip-products/processors/cortex-m, 2020.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bai et~al.(2020)Bai, Zhang, Hou, Shang, Jin, Jiang, Liu, Lyu, and
  King]{bai2020binarybert}
Bai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., Liu, Q., Lyu, M.,
  and King, I.
\newblock Binarybert: Pushing the limit of bert quantization.
\newblock \emph{arXiv preprint arXiv:2012.15701}, 2020.

\bibitem[Bhandare et~al.(2019)Bhandare, Sripathi, Karkada, Menon, Choi, Datta,
  and Saletore]{bhandare2019efficient}
Bhandare, A., Sripathi, V., Karkada, D., Menon, V., Choi, S., Datta, K., and
  Saletore, V.
\newblock Efficient 8-bit quantization of transformer neural machine language
  translation model.
\newblock \emph{arXiv preprint arXiv:1906.00532}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and
  Specia]{cer2017semeval}
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L.
\newblock Semeval-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock \emph{arXiv preprint arXiv:1708.00055}, 2017.

\bibitem[Chen et~al.(2018)Chen, Moreau, Jiang, Zheng, Yan, Shen, Cowan, Wang,
  Hu, Ceze, et~al.]{chen2018tvm}
Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan, M., Wang,
  L., Hu, Y., Ceze, L., et~al.
\newblock {TVM}: An automated end-to-end optimizing compiler for deep learning.
\newblock In \emph{13th {USENIX} Symposium on Operating Systems Design and
  Implementation ({OSDI} 18)}, pp.\  578--594, 2018.

\bibitem[Choi et~al.(2018)Choi, Wang, Venkataramani, Chuang, Srinivasan, and
  Gopalakrishnan]{choi2018pact}
Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srinivasan, V., and
  Gopalakrishnan, K.
\newblock {PACT}: Parameterized clipping activation for quantized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1805.06085}, 2018.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Binary{C}onnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3123--3131, 2015.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and
  Bengio]{courbariaux2016binarized}
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to+ 1 or-1.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Crandall \& Pomerance(2006)Crandall and Pomerance]{crandall2006prime}
Crandall, R. and Pomerance, C.~B.
\newblock \emph{Prime numbers: a computational perspective}, volume 182.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Dagan, I., Glickman, O., and Magnini, B.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine Learning Challenges Workshop}, pp.\  177--190.
  Springer, 2005.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, {\L}.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Detrey \& de~Dinechin(2005)Detrey and
  de~Dinechin]{detrey2005parameterized}
Detrey, J. and de~Dinechin, F.
\newblock A parameterized floating-point exponential function for fpgas.
\newblock In \emph{Proceedings. 2005 IEEE International Conference on
  Field-Programmable Technology, 2005.}, pp.\  27--34. IEEE, 2005.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dodge et~al.(2020)Dodge, Ilharco, Schwartz, Farhadi, Hajishirzi, and
  Smith]{dodge2020fine}
Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H., and Smith,
  N.
\newblock Fine-tuning pretrained language models: Weight initializations, data
  orders, and early stopping.
\newblock \emph{arXiv preprint arXiv:2002.06305}, 2020.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Dolan, W.~B. and Brockett, C.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on
  Paraphrasing (IWP2005)}, 2005.

\bibitem[Dong et~al.(2019)Dong, Yao, Gholami, Mahoney, and
  Keutzer]{dong2019hawq}
Dong, Z., Yao, Z., Gholami, A., Mahoney, M.~W., and Keutzer, K.
\newblock {HAWQ}: Hessian aware quantization of neural networks with
  mixed-precision.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  293--302, 2019.

\bibitem[Esser et~al.(2019)Esser, McKinstry, Bablani, Appuswamy, and
  Modha]{esser2019learned}
Esser, S.~K., McKinstry, J.~L., Bablani, D., Appuswamy, R., and Modha, D.~S.
\newblock Learned step size quantization.
\newblock \emph{arXiv preprint arXiv:1902.08153}, 2019.

\bibitem[Fan et~al.(2019)Fan, Grave, and Joulin]{fan2019reducing}
Fan, A., Grave, E., and Joulin, A.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock \emph{arXiv preprint arXiv:1909.11556}, 2019.

\bibitem[Fan et~al.(2020)Fan, Stock, Graham, Grave, Gribonval, Jegou, and
  Joulin]{fan2020training}
Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R., Jegou, H., and
  Joulin, A.
\newblock Training with quantization noise for extreme fixed-point compression.
\newblock \emph{arXiv preprint arXiv:2004.07320}, 2020.

\bibitem[Gholami et~al.(2018)Gholami, Kwon, Wu, Tai, Yue, Jin, Zhao, and
  Keutzer]{gholami2018squeezenext}
Gholami, A., Kwon, K., Wu, B., Tai, Z., Yue, X., Jin, P., Zhao, S., and
  Keutzer, K.
\newblock {SqueezeNext}: Hardware-aware neural network design.
\newblock \emph{Workshop paper in CVPR}, 2018.

\bibitem[Gordon et~al.(2020)Gordon, Duh, and Andrews]{gordon2020compressing}
Gordon, M.~A., Duh, K., and Andrews, N.
\newblock Compressing bert: Studying the effects of weight pruning on transfer
  learning.
\newblock \emph{arXiv preprint arXiv:2002.08307}, 2020.

\bibitem[Han \& Dally(2017)Han and Dally]{han2017efficient}
Han, S. and Dally, B.
\newblock Efficient methods and hardware for deep learning.
\newblock \emph{University Lecture}, 2017.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1135--1143, 2015.

\bibitem[Hauser \& Purdy(2001)Hauser and Purdy]{hauser2001approximating}
Hauser, J.~W. and Purdy, C.~N.
\newblock Approximating functions for embedded and asic applications.
\newblock In \emph{Proceedings of the 44th IEEE 2001 Midwest Symposium on
  Circuits and Systems. MWSCAS 2001 (Cat. No. 01CH37257)}, volume~1, pp.\
  478--481. IEEE, 2001.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units ({GELU}s).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hinton et~al.(2014)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{Workshop paper in NIPS}, 2014.

\bibitem[Howard et~al.(2019)Howard, Sandler, Chu, Chen, Chen, Tan, Wang, Zhu,
  Pang, Vasudevan, et~al.]{howard2019searching}
Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W.,
  Zhu, Y., Pang, R., Vasudevan, V., et~al.
\newblock Searching for {MobilenetV3}.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1314--1324, 2019.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{iandola2016squeezenet}
Iandola, F.~N., Han, S., Moskewicz, M.~W., Ashraf, K., Dally, W.~J., and
  Keutzer, K.
\newblock {SqueezeNet}: Alexnet-level accuracy with 50x fewer parameters and<
  0.5 mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Iyer et~al.(2017)Iyer, Dandekar, and Csernai]{iyer2017first}
Iyer, S., Dandekar, N., and Csernai, K.
\newblock First quora dataset release: Question pairs.(2017).
\newblock \emph{URL https://data. quora.
  com/First-Quora-Dataset-Release-Question-Pairs}, 2017.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{jacob2018quantization}
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and
  Kalenichenko, D.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2704--2713, 2018.

\bibitem[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{jiao2019tinybert}
Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu,
  Q.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1909.10351}, 2019.

\bibitem[Jin et~al.(2021)Jin, Liang, Wu, Zou, and Gan]{jin2021kdlsq}
Jin, J., Liang, C., Wu, T., Zou, L., and Gan, Z.
\newblock Kdlsq-bert: A quantized bert combining knowledge distillation with
  learned step size quantization.
\newblock \emph{arXiv preprint arXiv:2101.05938}, 2021.

\bibitem[Kim(2021)]{kim2021ibert}
Kim, S.
\newblock https://github.com/kssteven418/i-bert, 2021.

\bibitem[Krishnamoorthi(2018)]{krishnamoorthi2018quantizing}
Krishnamoorthi, R.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper.
\newblock \emph{arXiv preprint arXiv:1806.08342}, 2018.

\bibitem[Kwon et~al.(2018)Kwon, Amid, Gholami, Wu, Asanovic, and
  Keutzer]{kwon2018co}
Kwon, K., Amid, A., Gholami, A., Wu, B., Asanovic, K., and Keutzer, K.
\newblock Co-design of deep neural nets and neural net accelerators for
  embedded vision applications.
\newblock In \emph{2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)},
  pp.\  1--6. IEEE, 2018.

\bibitem[Lai et~al.(2018)Lai, Suda, and Chandra]{lai2018cmsis}
Lai, L., Suda, N., and Chandra, V.
\newblock {CMSIS-NN}: Efficient neural network kernels for arm cortex-m cpus.
\newblock \emph{arXiv preprint arXiv:1801.06601}, 2018.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{lepikhin2020gshard}
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M.,
  Shazeer, N., and Chen, Z.
\newblock {GShard}: Scaling giant models with conditional computation and
  automatic sharding.
\newblock \emph{arXiv preprint arXiv:2006.16668}, 2020.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and
  Morgenstern]{levesque2012winograd}
Levesque, H., Davis, E., and Morgenstern, L.
\newblock The winograd schema challenge.
\newblock In \emph{Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}. Citeseer, 2012.

\bibitem[Li et~al.(2016{\natexlab{a}})Li, Zhang, and Liu]{li2016ternary}
Li, F., Zhang, B., and Liu, B.
\newblock Ternary weight networks.
\newblock \emph{arXiv preprint arXiv:1605.04711}, 2016{\natexlab{a}}.

\bibitem[Li et~al.(2016{\natexlab{b}})Li, Kadav, Durdanovic, Samet, and
  Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock {RoBERTa}: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Mao et~al.(2017)Mao, Han, Pool, Li, Liu, Wang, and
  Dally]{mao2017exploring}
Mao, H., Han, S., Pool, J., Li, W., Liu, X., Wang, Y., and Dally, W.~J.
\newblock Exploring the regularity of sparse structure in convolutional neural
  networks.
\newblock \emph{Workshop paper in CVPR}, 2017.

\bibitem[Mao et~al.(2020)Mao, Wang, Wu, Zhang, Wang, Yang, Zhang, Tong, and
  Bai]{mao2020ladabert}
Mao, Y., Wang, Y., Wu, C., Zhang, C., Wang, Y., Yang, Y., Zhang, Q., Tong, Y.,
  and Bai, J.
\newblock Ladabert: Lightweight adaptation of bert through hybrid model
  compression.
\newblock \emph{arXiv preprint arXiv:2004.04124}, 2020.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{michel2019sixteen}
Michel, P., Levy, O., and Neubig, G.
\newblock Are sixteen heads really better than one?
\newblock \emph{arXiv preprint arXiv:1905.10650}, 2019.

\bibitem[Mishra \& Marr(2017)Mishra and Marr]{mishra2017apprentice}
Mishra, A. and Marr, D.
\newblock Apprentice: Using knowledge distillation techniques to improve
  low-precision network accuracy.
\newblock \emph{arXiv preprint arXiv:1711.05852}, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Mukherjee et~al.(2019)Mukherjee, Weill, Taneja, Onofrio, Ko, and
  Sharma]{tensorrtbert}
Mukherjee, P., Weill, E., Taneja, R., Onofrio, D., Ko, Y.-J., and Sharma, S.
\newblock Real-time natural language understanding with bert using tensorrt,
  hhttps://developer.nvidia.com/blog/nlu-with-tensorrt-bert/, 2019.

\bibitem[{NVIDIA}(2018)]{tensorrt}
{NVIDIA}.
\newblock Tensor{RT}: https://developer.nvidia.com/tensorrt, 2018.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock {FairSeq}: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.

\bibitem[Park et~al.(2018)Park, Yoo, and Vajda]{park2018value}
Park, E., Yoo, S., and Vajda, P.
\newblock Value-aware quantization for training and inference of neural
  networks.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  580--595, 2018.

\bibitem[Polino et~al.(2018)Polino, Pascanu, and Alistarh]{polino2018model}
Polino, A., Pascanu, R., and Alistarh, D.
\newblock Model compression via distillation and quantization.
\newblock \emph{arXiv preprint arXiv:1802.05668}, 2018.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Raganato et~al.(2020)Raganato, Scherrer, and
  Tiedemann]{raganato2020fixed}
Raganato, A., Scherrer, Y., and Tiedemann, J.
\newblock Fixed encoder self-attention patterns in transformer-based machine
  translation.
\newblock \emph{arXiv preprint arXiv:2002.10260}, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock {XNOR-Net}: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, pp.\  525--542.
  Springer, 2016.

\bibitem[Romero et~al.(2014)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
Romero, A., Ballas, N., Kahou, S.~E., Chassang, A., Gatta, C., and Bengio, Y.
\newblock {FitNet}s: Hints for thin deep nets.
\newblock \emph{arXiv preprint arXiv:1412.6550}, 2014.

\bibitem[Rosset(2019)]{rosset2019turing}
Rosset, C.
\newblock Turing-{NLG}: A 17-billion-parameter language model by microsoft.
\newblock \emph{Microsoft Blog}, 2019.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock {MobilenetV2}: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4510--4520, 2018.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Sanh, V., Wolf, T., and Rush, A.~M.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{arXiv preprint arXiv:2005.07683}, 2020.

\bibitem[Schraudolph(1999)]{schraudolph1999fast}
Schraudolph, N.~N.
\newblock A fast, compact approximation of the exponential function.
\newblock \emph{Neural Computation}, 11\penalty0 (4):\penalty0 853--862, 1999.

\bibitem[Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer]{shen2020q}
Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M.~W., and
  Keutzer, K.
\newblock {Q-BERT}: Hessian based ultra low precision quantization of bert.
\newblock In \emph{AAAI}, pp.\  8815--8821, 2020.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-{LM}: Training multi-billion parameter language models using
  gpu model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and
  Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pp.\  1631--1642, 2013.

\bibitem[Stewart(1996)]{stewart1996afternotes}
Stewart, G.~W.
\newblock \emph{Afternotes on numerical analysis}.
\newblock SIAM, 1996.

\bibitem[Sun et~al.(2019)Sun, Cheng, Gan, and Liu]{sun2019patient}
Sun, S., Cheng, Y., Gan, Z., and Liu, J.
\newblock Patient knowledge distillation for bert model compression.
\newblock \emph{arXiv preprint arXiv:1908.09355}, 2019.

\bibitem[Sun et~al.(2020)Sun, Yu, Song, Liu, Yang, and Zhou]{sun2020mobilebert}
Sun, Z., Yu, H., Song, X., Liu, R., Yang, Y., and Zhou, D.
\newblock Mobilebert: a compact task-agnostic bert for resource-limited
  devices.
\newblock \emph{arXiv preprint arXiv:2004.02984}, 2020.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.~V.
\newblock Efficient{N}et: Rethinking model scaling for convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1905.11946}, 2019.

\bibitem[Tang et~al.(2019)Tang, Lu, Liu, Mou, Vechtomova, and
  Lin]{tang2019distilling}
Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin, J.
\newblock Distilling task-specific knowledge from bert into simple neural
  networks.
\newblock \emph{arXiv preprint arXiv:1903.12136}, 2019.

\bibitem[Thomas et~al.(2004)Thomas, Okada, Markstein, and Li]{thomas2004libm}
Thomas, J.~W., Okada, J.~P., Markstein, P., and Li, R.-C.
\newblock The libm library and floatingpoint arithmetic in hp-ux for
  itanium-based systems.
\newblock Technical report, Technical report, Hewlett-Packard Company, Palo
  Alto, CA, USA, 2004.

\bibitem[Turc et~al.(2019)Turc, Chang, Lee, and Toutanova]{turc2019well}
Turc, I., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Well-read students learn better: On the importance of pre-training
  compact models.
\newblock \emph{arXiv preprint arXiv:1908.08962}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2019)Wang, Liu, Lin, Lin, and Han]{wang2018haq}
Wang, K., Liu, Z., Lin, Y., Lin, J., and Han, S.
\newblock {HAQ}: Hardware-aware automated quantization.
\newblock \emph{In Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2019.

\bibitem[Wang et~al.(2020)Wang, Wei, Dong, Bao, Yang, and Zhou]{wang2020minilm}
Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2002.10957}, 2020.

\bibitem[Waring(1779)]{waring1779vii}
Waring, E.
\newblock Vii. problems concerning interpolations.
\newblock \emph{Philosophical transactions of the royal society of London},
  1779.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt2019neural}
Warstadt, A., Singh, A., and Bowman, S.~R.
\newblock Neural network acceptability judgments.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 625--641, 2019.

\bibitem[Williams et~al.(2017)Williams, Nangia, and Bowman]{williams2017broad}
Williams, A., Nangia, N., and Bowman, S.~R.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock \emph{arXiv preprint arXiv:1704.05426}, 2017.

\bibitem[Wu et~al.(2018)Wu, Wang, Zhang, Tian, Vajda, and Keutzer]{wu2018mixed}
Wu, B., Wang, Y., Zhang, P., Tian, Y., Vajda, P., and Keutzer, K.
\newblock Mixed precision quantization of convnets via differentiable neural
  architecture search.
\newblock \emph{arXiv preprint arXiv:1812.00090}, 2018.

\bibitem[Wu et~al.(2016)Wu, Leng, Wang, Hu, and Cheng]{wu2016quantized}
Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J.
\newblock Quantized convolutional neural networks for mobile devices.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4820--4828, 2016.

\bibitem[Xu et~al.(2020)Xu, Zhou, Ge, Wei, and Zhou]{xu2020bert}
Xu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M.
\newblock Bert-of-theseus: Compressing bert by progressive module replacing.
\newblock \emph{arXiv preprint arXiv:2002.02925}, 2020.

\bibitem[Yang et~al.(2017)Yang, Chen, and Sze]{yang2017designing}
Yang, T.-J., Chen, Y.-H., and Sze, V.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5687--5695, 2017.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.~R., and Le, Q.~V.
\newblock {XLNet}: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5753--5763, 2019.

\bibitem[Yao et~al.(2020)Yao, Dong, Zheng, Gholami, Yu, Tan, Wang, Huang, Wang,
  Mahoney, and Keutzer]{yao2020hawqv3}
Yao, Z., Dong, Z., Zheng, Z., Gholami, A., Yu, J., Tan, E., Wang, L., Huang,
  Q., Wang, Y., Mahoney, M.~W., and Keutzer, K.
\newblock {HAWQV3}: Dyadic neural network quantization.
\newblock \emph{arXiv preprint arXiv:2011.10680}, 2020.

\bibitem[Zadeh et~al.(2020)Zadeh, Edo, Awad, and Moshovos]{zadeh2020gobo}
Zadeh, A.~H., Edo, I., Awad, O.~M., and Moshovos, A.
\newblock Gobo: Quantizing attention-based nlp models for low latency and
  energy efficient inference.
\newblock In \emph{2020 53rd Annual IEEE/ACM International Symposium on
  Microarchitecture (MICRO)}, pp.\  811--824. IEEE, 2020.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat]{zafrir2019q8bert}
Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M.
\newblock {Q8BERT}: Quantized 8bit bert.
\newblock \emph{arXiv preprint arXiv:1910.06188}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Yang, Ye, and Hua]{zhang2018lq}
Zhang, D., Yang, J., Ye, D., and Hua, G.
\newblock {LQ-Nets}: Learned quantization for highly accurate and compact deep
  neural networks.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  365--382, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Hou, Yin, Shang, Chen, Jiang, and
  Liu]{zhang2020ternarybert}
Zhang, W., Hou, L., Yin, Y., Shang, L., Chen, X., Jiang, X., and Liu, Q.
\newblock Ternarybert: Distillation-aware ultra-low bit bert.
\newblock \emph{arXiv preprint arXiv:2009.12812}, 2020.

\bibitem[Zhou et~al.(2016)Zhou, Wu, Ni, Zhou, Wen, and Zou]{zhou2016dorefa}
Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
\newblock {DoReFa-Net}: Training low bitwidth convolutional neural networks
  with low bitwidth gradients.
\newblock \emph{arXiv preprint arXiv:1606.06160}, 2016.

\end{thebibliography}
